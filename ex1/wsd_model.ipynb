{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KBQZqlCFgTN"
   },
   "source": [
    "## Studets details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC21v1qDFgTT"
   },
   "source": [
    "Student1\n",
    "* Name:\n",
    "* ID:\n",
    "* Username:\n",
    "\n",
    "Student2\n",
    "* Name:\n",
    "* ID:\n",
    "* Username:\n",
    "\n",
    "Student3\n",
    "* Name:\n",
    "* ID:\n",
    "* Username:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8onGgw0UFgTU"
   },
   "source": [
    "### General tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVAW7FkoFgTU"
   },
   "source": [
    "While debugging you might want to use:\n",
    "```python\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "```\n",
    "\n",
    "to reload the model module without repeating unnecessary cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNumumJbF7I2",
    "outputId": "5d78504c-e165-44cc-c0c1-4ca986451d4c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnOa3RWWFgTU"
   },
   "source": [
    "### Import relevant packages - you might need to pip install some "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h65x2IjrFgTV",
    "outputId": "21972f66-f864-4917-a6c8-dd63dc10f461"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from os.path import dirname\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "sys.path.append('/content/drive/MyDrive/TAU/Advanced NLP/Ex1')\n",
    "\n",
    "import data_loader\n",
    "from traineval import train, evaluate\n",
    "import model as model\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"deviced used is {device}\")\n",
    "\n",
    "import importlib\n",
    "importlib.reload(model)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBofcts7FgTV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVe0N-Y_FgTV"
   },
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3A94olsFgTV"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "The following line of code invokes data_loader and will automatically download and extract the dataset if needed.\n",
    "It instantiates the following variables;\n",
    "* tokens_vocab - the sentence words vocabulary\n",
    "* y_vocab - the labels (senses) vocabulary\n",
    "* datasets - a dictionary with train,dev, and test WSDDataset instances.\n",
    "\n",
    "Use the optional sentence_count kwarg to limit the number of sentences loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkOMi9JYFgTY",
    "outputId": "f00bffcd-445e-43fc-a08c-6f0a0b6c666e"
   },
   "outputs": [],
   "source": [
    "train_dataset, tokens_vocab, y_vocab = data_loader.load_train_dataset()\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJWDc1mbFgTY",
    "outputId": "07b25ae2-8631-4277-c3a8-44f17c698c2a"
   },
   "outputs": [],
   "source": [
    "dev_dataset = data_loader.load_dev_dataset(tokens_vocab, y_vocab)\n",
    "dev_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hqN7EajFgTY"
   },
   "source": [
    "## Part 1: Query-Based Attention\n",
    "\n",
    "Implement the relevant parts in model.py module. You might to check out this blog post about [gather method](https://medium.com/analytics-vidhya/understanding-indexing-with-pytorch-gather-33717a84ebc4)\n",
    "\n",
    "Load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ch84G2mFgTY",
    "outputId": "dca8afdc-d778-4262-8ea2-df69c2eb60f1"
   },
   "outputs": [],
   "source": [
    "dropout = 0.25\n",
    "D = 300\n",
    "lr = 8e-5\n",
    "batch_size=100\n",
    "num_epochs=5\n",
    "set_seed(seed)\n",
    "\n",
    "m = model.WSDModel(\n",
    "    tokens_vocab.size(), \n",
    "    y_vocab.size(), \n",
    "    D=D, \n",
    "    dropout_prob=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "losses, train_acc, val_acc = train(m, \n",
    "                                   optimizer, \n",
    "                                   train_dataset, \n",
    "                                   dev_dataset, \n",
    "                                   num_epochs=num_epochs, \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPaXZRiFFgTZ"
   },
   "source": [
    "Train the model - you shoud see the loss decreasing and validation acc increasing from epoch to epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1ppPREcFgTZ",
    "outputId": "099d0406-261f-41e8-8f35-373723135a50"
   },
   "outputs": [],
   "source": [
    "print(f\"Validation accuracy: {val_acc[-1]:.3f}, Training accuracy:{train_acc[-1]:.3f}\")\n",
    "assert round(val_acc[-1], 3) >= 0.514, \"The last validation accuracy should be at least 0.514. Please check your implementation before you continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik_A5xRwFgTZ"
   },
   "source": [
    "Plot the loss and training/validation accuracy. You should be getting ~54% validation accuracy after 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "vP2g3IFaFgTZ",
    "outputId": "dd80cf68-5e5f-4abd-aad6-96a79b84c8f0"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses, '-', label='Train Loss');\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS1c-JGRFgTa"
   },
   "source": [
    "Use the attention vizualization to get a feel of what the model is attending to.\n",
    "\n",
    "The query token is highlighted in green, and the model's attention with a pink-blue gradient.\n",
    "In addition, the loss is given a red gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "TkHUYG-9FgTa",
    "outputId": "a0e8f5d4-ab87-497d-ddaf-4b18968fc29a"
   },
   "outputs": [],
   "source": [
    "from traineval import higlight_samples\n",
    "\n",
    "higlight_samples(m, dev_dataset, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZ56RzymFgTb"
   },
   "source": [
    "## Part 2: Padding\n",
    "\n",
    "Implement the padding mask in the attention function in model.py.\n",
    "\n",
    "Load the model and retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLRlMXHlFgTb",
    "outputId": "d6e760bc-419f-40af-ee3c-cc4cc2d5ec2f"
   },
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "m = model.WSDModel(\n",
    "    tokens_vocab.size(), \n",
    "    y_vocab.size(), \n",
    "    D=D, \n",
    "    dropout_prob=dropout,\n",
    "    use_padding=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "losses, train_acc, val_acc = train(\n",
    "    m, optimizer, train_dataset, dev_dataset, num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHnZLJ7OFgTb",
    "outputId": "3067a482-c791-41dd-8290-30f9c9e8c9ec"
   },
   "outputs": [],
   "source": [
    "print(f\"Validation accuracy: {val_acc[-1]:.3f}, Training accuracy:{train_acc[-1]:.3f}\")\n",
    "assert round(val_acc[-1], 3) >= 0.527, \"The last validation accuracy should be at least 0.527. Please check your implementation before you continue\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC5JIFIOFgTb"
   },
   "source": [
    "Plot the loss and training/validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "NUR0dgZkFgTc",
    "outputId": "6eb967b7-d170-463c-8c3a-cccd3f6f7c24"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses, '-', label='Train Loss');\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "XjPaUNd3FgTc",
    "outputId": "a36eec59-3b09-43cc-b7bd-0657ee459c4b"
   },
   "outputs": [],
   "source": [
    "higlight_samples(m, dev_dataset, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC3LNYrAFgTc"
   },
   "source": [
    "Examine additional examples, using the API and pandas as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRl63fKJFgTc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from traineval import evaluate_verbose, highlight\n",
    "\n",
    "pd.set_option('max_columns', 100)\n",
    "\n",
    "eval_df, attention_df = evaluate_verbose(m, dev_dataset, iter_lim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfOoaTutFgTc"
   },
   "source": [
    "Visualization of 5 incorrectly classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "nBLK54OmFgTd",
    "outputId": "c51d4352-3a01-4367-87dc-e9c6462ae959"
   },
   "outputs": [],
   "source": [
    "idxs = np.where(eval_df['y_true'] != eval_df['y_pred'])\n",
    "idxs = list(idxs[0][:5])\n",
    "highlight(eval_df, attention_df, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7MTA7RlFgTd"
   },
   "source": [
    "Visualization of examples with the query word \"left\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "B3VjW-KnFgTd",
    "outputId": "a2e456b3-9bee-4985-bad5-36d839078d57"
   },
   "outputs": [],
   "source": [
    "idxs = np.where(eval_df['query_token'] == 'left')\n",
    "highlight(eval_df, attention_df, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5HSKasTFgTd"
   },
   "source": [
    "## Part 3: Self-Attention\n",
    "\n",
    "The method below converts the query-based instances in WSDDataset to sentence-level instances in WSDSentencesDataset for self-attention.\n",
    "\n",
    "Notice how the number of samples now equals number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3x8YzfYDFgTd",
    "outputId": "ab354cac-bb90-49e1-dd0c-0edf739ca8c1"
   },
   "outputs": [],
   "source": [
    "sa_train_dataset = data_loader.WSDSentencesDataset.from_word_dataset(train_dataset)\n",
    "sa_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2c4DyoyFgTe",
    "outputId": "d776f0f2-1425-4314-bd06-f21599dd2244"
   },
   "outputs": [],
   "source": [
    "sa_dev_dataset = data_loader.WSDSentencesDataset.from_word_dataset(dev_dataset)\n",
    "sa_dev_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ2oDbtCFgTe"
   },
   "source": [
    "Implement self-attention in the model.\n",
    "\n",
    "Load the model and retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltxWhZMrFgTe",
    "outputId": "3cd482de-55d2-4823-9b19-66a4b96e43ed"
   },
   "outputs": [],
   "source": [
    "lr=2e-4\n",
    "dropout = 0.2\n",
    "D=300\n",
    "batch_size=20\n",
    "num_epochs=2\n",
    "set_seed(seed)\n",
    "\n",
    "m = model.WSDModel(\n",
    "    tokens_vocab.size(), \n",
    "    y_vocab.size(), \n",
    "    D=D, \n",
    "    dropout_prob=dropout,\n",
    "    use_padding=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "losses, train_acc, val_acc = train(\n",
    "    m, optimizer, sa_train_dataset, sa_dev_dataset, num_epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBdJ1g1MFgTe"
   },
   "source": [
    "Plot the loss and training/validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "MwXsJOblFgTe",
    "outputId": "b431c4d4-a148-49cf-f8fd-7d8204bbe4b7"
   },
   "outputs": [],
   "source": [
    "print(f\"Validation accuracy: {val_acc[-1]:.3f}, Training accuracy:{train_acc[-1]:.3f}\")\n",
    "# assert val_acc[-1] >= 0.543, \"The last validation accuracy should be at least 0.543. Please check your implementation before you continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3t0LR4AFgTe"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "axs[0].plot(losses, '-', label='Train Loss');\n",
    "axs[0].legend()\n",
    "axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUHC4OFxFgTe"
   },
   "source": [
    "## Part 4: Positional embeddings  &  Part 5: Causal Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIqkFs9qFgTf"
   },
   "source": [
    "We do not provide \"you code here\" comments for this part as you should be familiar with the code by now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "\n",
    "def train_eval_positional(num_epochs: int,\n",
    "                          pos_exponent: int,\n",
    "                          pos_cutoff_position: int,\n",
    "                          pos_is_causal:bool,\n",
    "                          pos_normalize_magnitude: bool,\n",
    "                          DEBUG_dummy_train: bool = False) -> None:\n",
    "    \n",
    "    sa_train_dataset = data_loader.WSDSentencesDataset.from_word_dataset(train_dataset)\n",
    "    sa_dev_dataset = data_loader.WSDSentencesDataset.from_word_dataset(dev_dataset)\n",
    "\n",
    "    if DEBUG_dummy_train:\n",
    "        sa_train_dataset = sa_dev_dataset\n",
    "        warn(\"using dev set for training\")\n",
    "        print()\n",
    "    \n",
    "    lr=2e-4\n",
    "    dropout = 0.2\n",
    "    D=300\n",
    "    batch_size=20\n",
    "    set_seed(seed)\n",
    "\n",
    "    m = model.WSDModel(\n",
    "        tokens_vocab.size(), \n",
    "        y_vocab.size(), \n",
    "        D=D, \n",
    "        dropout_prob=dropout,\n",
    "        use_padding=True,\n",
    "        use_positional_encodings=True,\n",
    "        pos_exponent=pos_exponent,\n",
    "        pos_cutoff_position=pos_cutoff_position,\n",
    "        pos_is_causal=pos_is_causal,\n",
    "        pos_normalize_magnitude=pos_normalize_magnitude\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "    losses, train_acc, val_acc = train(\n",
    "        m, optimizer, sa_train_dataset, sa_dev_dataset, num_epochs=num_epochs, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Validation accuracy: {val_acc[-1]:.3f}, Training accuracy:{train_acc[-1]:.3f}\")\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, figsize=(15, 6))\n",
    "\n",
    "    axs[0].plot(losses, '-', label='Train Loss');\n",
    "    axs[0].legend()\n",
    "    axs[1].plot(train_acc, '-o', label='Train Acc');\n",
    "    axs[1].plot(val_acc, '-o', label='Val Acc');\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfm8b-fYFgTf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for pos_is_causal in [False, True]:\n",
    "    for pos_normalize_magnitude in [False, True]:\n",
    "        for pos_exponent in [1, 1.5, 2]:\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"==================================================================================\")\n",
    "            print(\"pos_is_causal:\", pos_is_causal, \" | pos_normalize_magnitude:\", pos_normalize_magnitude,\n",
    "                  \" | pos_exponent:\", pos_exponent)\n",
    "            print(\"==================================================================================\")\n",
    "            train_eval_positional(num_epochs=1,\n",
    "                                  pos_exponent=pos_exponent,\n",
    "                                  pos_cutoff_position=10,\n",
    "                                  pos_is_causal=pos_is_causal,\n",
    "                                  pos_normalize_magnitude=pos_normalize_magnitude,\n",
    "                                  DEBUG_dummy_train=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wsd_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tomer",
   "language": "python",
   "name": "tomer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
